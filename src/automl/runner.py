#!/usr/bin/env python
"""
AutoML Runner Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LightAutoML Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ JSON-Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°.

ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Data-Agent, Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ CSV Ğ¸Ğ· splits/,
Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ LightAutoML Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.
"""

from __future__ import annotations

import json
import sys
import warnings
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Literal, Optional, Type

import pandas as pd
from sklearn.model_selection import train_test_split

PROJECT_ROOT = Path(__file__).resolve().parents[2]
EXTERNAL_LAMA = PROJECT_ROOT / "external_libs" / "lightautoml"
if EXTERNAL_LAMA.exists() and str(EXTERNAL_LAMA) not in sys.path:
    sys.path.insert(0, str(EXTERNAL_LAMA))

try:
    from lightautoml.automl.presets.tabular_presets import TabularAutoML
    from lightautoml.tasks import Task
except ImportError as exc:
    raise ImportError(
        "ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ LightAutoML. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ, ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ»Ğ¸ Ğ¿Ğ°ĞºĞµÑ‚ Ğ¸ PyTorch."
    ) from exc

try:
    from lightautoml.utils import create_leaderboard  # Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² ÑÑ‚Ğ°Ñ€Ñ‹Ñ… Ğ²ĞµÑ€ÑĞ¸ÑÑ…
except ImportError:
    create_leaderboard = None


PROCESSED_DIR = PROJECT_ROOT / "data" / "processed"
REPORTS_DIR = PROJECT_ROOT / "reports"
VariantName = Literal["baseline", "research"]


def load_metadata(dataset_name: str) -> Dict[str, Any]:
    """Ğ§Ğ¸Ñ‚Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ñ‚Ñƒ Data-Agent Ğ´Ğ»Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°."""
    metadata_path = REPORTS_DIR / f"{dataset_name}_metadata.json"
    if not metadata_path.exists():
        raise FileNotFoundError(f"ĞœĞµÑ‚Ğ°Ğ´Ğ°Ñ‚Ğ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°: {metadata_path}")
    
    with metadata_path.open("r", encoding="utf-8") as f:
        return json.load(f)


def load_splits(dataset_name: str, splits_info: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ train/val/test CSV Ğ¸Ğ· splits/."""
    files = splits_info.get("files", {})
    result = {}
    
    for split_name in ["train", "val", "test"]:
        if split_name not in files:
            warnings.warn(f"ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ {split_name} split Ğ´Ğ»Ñ {dataset_name}")
            continue
        
        path = Path(files[split_name])
        if not path.exists():
            warnings.warn(f"Ğ¤Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {path}")
            continue
        
        result[split_name] = pd.read_csv(path)
    
    return result


def _resolve_preset_class(variant: VariantName) -> Type[TabularAutoML]:
    if variant == "research":
        try:
            from lightautoml.automl.presets.tabular_presets import ResearchTabularAutoML

            return ResearchTabularAutoML
        except ImportError:
            warnings.warn("ResearchTabularAutoML Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ TabularAutoML.")
    return TabularAutoML


def train_automl(
    train_df: pd.DataFrame,
    target_column: str,
    task_mode: str,
    timeout: Optional[int] = 600,
    verbose: bool = True,
    variant: VariantName = "baseline",
) -> tuple[TabularAutoML, Any]:
    """
    ĞĞ±ÑƒÑ‡Ğ°ĞµÑ‚ TabularAutoML Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ + OOF-Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹.
    
    Args:
        train_df: Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚
        target_column: Ğ¸Ğ¼Ñ Ñ‚Ğ°Ñ€Ğ³ĞµÑ‚Ğ°
        task_type: Ñ‚Ğ¸Ğ¿ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ (classification/regression)
        timeout: Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞµĞºÑƒĞ½Ğ´Ğ°Ñ…
    
    Returns:
        automl: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµÑĞµÑ‚
        oof_predictions: out-of-fold Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ leaderboard
    """
    if target_column not in train_df.columns:
        raise KeyError(f"Ğ¡Ñ‚Ğ¾Ğ»Ğ±ĞµÑ† Ñ‚Ğ°Ñ€Ğ³ĞµÑ‚Ğ° '{target_column}' Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    task = Task(task_mode)

    preset_cls = _resolve_preset_class(variant)
    reader_params = {"n_jobs": 1, "advanced_roles": False}
    selection_params = {"mode": 0}
    tuning_params = {"max_tuning_iter": 0, "max_tuning_time": 0}
    automl = preset_cls(
        task=task,
        timeout=timeout,
        reader_params=reader_params,
        selection_params=selection_params,
        tuning_params=tuning_params,
    )

    if verbose:
        print(f"â–¶ï¸  Ğ—Ğ°Ğ¿ÑƒÑĞº LightAutoML ({task_mode}, timeout={timeout}s)...")
    oof_predictions = automl.fit_predict(train_df, roles={"target": target_column})
    
    return automl, oof_predictions


def evaluate_model(
    automl: TabularAutoML,
    test_df: pd.DataFrame,
    target_column: str,
) -> Dict[str, float]:
    """ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ LightAutoML."""
    if target_column not in test_df.columns:
        raise KeyError(f"Ğ¡Ñ‚Ğ¾Ğ»Ğ±ĞµÑ† Ñ‚Ğ°Ñ€Ğ³ĞµÑ‚Ğ° '{target_column}' Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….")

    missing_mask = test_df[target_column].isna()
    if missing_mask.any():
        removed = int(missing_mask.sum())
        warnings.warn(
            f"Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾ {removed} ÑÑ‚Ñ€Ğ¾Ğº Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ°Ñ€Ğ³ĞµÑ‚Ğ° '{target_column}' Ğ¿ĞµÑ€ĞµĞ´ Ñ€Ğ°ÑÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº."
        )
        test_df = test_df.loc[~missing_mask].reset_index(drop=True)
        if test_df.empty:
            warnings.warn("ĞŸĞ¾ÑĞ»Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ² Ñ‚Ğ°Ñ€Ğ³ĞµÑ‚Ğµ Ğ½Ğµ Ğ¾ÑÑ‚Ğ°Ğ»Ğ¾ÑÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸.")
            return {}

    metric_func = getattr(automl.task, "metric_func", None)
    metric_name = getattr(automl.task, "metric_name", None)

    if metric_func is None:
        warnings.warn("LightAutoML Task Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ metric_func â€” Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹.")
        return {}

    features = test_df.drop(columns=[target_column])
    target = test_df[target_column].values
    predictions = automl.predict(features).data
    if predictions.ndim > 1 and predictions.shape[1] == 1:
        predictions = predictions[:, 0]

    metrics: Dict[str, float] = {}
    try:
        score = metric_func(target, predictions)
        metric_label = str(metric_name or getattr(getattr(metric_func, "func", metric_func), "__name__", "metric"))
        metrics[metric_label] = float(score)
    except Exception as exc:
        warnings.warn(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ LightAutoML '{metric_name}': {exc}")

    return metrics


def create_report(
    dataset_name: str,
    metadata: Dict[str, Any],
    automl: TabularAutoML,
    oof_predictions: Any,
    test_metrics: Dict[str, float],
    timeout: Optional[int],
    variant: VariantName,
) -> Dict[str, Any]:
    """Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ JSON-Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ AutoML."""
    
    # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ leaderboard
    leaderboard_data = None
    best_model_info = {}
    
    try:
        if create_leaderboard:
            leaderboard = create_leaderboard(oof_predictions)
            if leaderboard is not None and not leaderboard.empty:
                leaderboard_data = leaderboard.to_dict(orient="records")
                best_model = leaderboard.iloc[0]
                best_model_info = {
                    "name": str(best_model.get("model", "unknown")),
                    "score": float(best_model.get("score", 0.0)) if best_model.get("score") is not None else None,
                }
    except Exception as e:
        warnings.warn(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ leaderboard Ñ‡ĞµÑ€ĞµĞ· lightautoml.utils: {e}")
    
    if leaderboard_data is None:
        fallback_entries = _build_fallback_leaderboard(automl)
        if fallback_entries:
            leaderboard_data = fallback_entries
            best_model_info = {
                "name": fallback_entries[0]["model"],
                "weight": fallback_entries[0]["weight"],
            }
    
    if not best_model_info:
        primary_metric = _primary_metric(metadata.get("task_type", "classification"))
        best_model_info = {
            "name": "LightAutoML Tabular Ensemble",
            "metric": primary_metric,
            "score": test_metrics.get(primary_metric),
        }

    if best_model_info.get("score") is None:
        metric_key = None
        metric_value = None
        if test_metrics:
            metric_key, metric_value = next(iter(test_metrics.items()))
        if metric_value is not None:
            best_model_info["score"] = metric_value
            if "metric" not in best_model_info:
                best_model_info["metric"] = metric_key

    report = {
        "dataset": dataset_name,
        "task_type": metadata.get("task_type", "unknown"),
        "target_column": metadata.get("target", "unknown"),
        "automl": {
            "data": {
                "train": metadata.get("auto_ml", {}).get("exports", {}).get("splits", {}).get("files", {}).get("train"),
                "test": metadata.get("auto_ml", {}).get("exports", {}).get("splits", {}).get("files", {}).get("test"),
            },
            "framework": "LightAutoML",
            "version": "0.4.1",  # Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ
            "variant": variant,
            "run": {
                "started_at": datetime.utcnow().isoformat() + "Z",
                "timeout_seconds": timeout,
                "variant": variant,
            },
            "best_model": best_model_info,
            "test_metrics": test_metrics,
            "leaderboard": leaderboard_data,
        },
        "data_processing": {
            "rows": metadata.get("rows"),
            "original_columns": metadata.get("original_columns"),
            "processed_features": metadata.get("processed_features"),
            "transformations": metadata.get("transformations", []),
        },
    }
    
    return report


def save_report(dataset_name: str, report: Dict[str, Any]) -> str:
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ Ğ² JSON-Ñ„Ğ°Ğ¹Ğ»."""
    output_dir = REPORTS_DIR
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / f"{dataset_name}_automl.json"
    
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    return str(output_path)


def run(
    dataset_name: str,
    timeout: Optional[int] = 600,
    verbose: bool = True,
    variant: VariantName = "baseline",
) -> Dict[str, Any]:
    """
    Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ: Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ AutoML Ğ´Ğ»Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°.
    
    Args:
        dataset_name: Ğ¸Ğ¼Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, 'titanic')
        timeout: Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞµĞºÑƒĞ½Ğ´Ğ°Ñ…
    
    Returns:
        ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ (Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñƒ, Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸ Ñ‚.Ğ´.)
    """
    if verbose:
        print(f"ğŸ“Š Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ {dataset_name}...")
    metadata = load_metadata(dataset_name)
    
    target_column = metadata.get("target")
    task_type = metadata.get("task_type", "classification")
    splits_info = metadata.get("auto_ml", {}).get("exports", {}).get("splits", {})
    
    if not target_column:
        raise ValueError("Ğ’ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ‚Ğ°Ñ€Ğ³ĞµÑ‚Ğµ")
    
    if verbose:
        print(f"ğŸ“‚ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° splits (train/val/test)...")
        splits = load_splits(dataset_name, splits_info)
    else:
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning)
            splits = load_splits(dataset_name, splits_info)
    
    if "train" not in splits:
        raise ValueError("ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ train split â€” Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ Data-Agent.")

    train_df = splits["train"]
    test_df = splits.get("test")

    if test_df is None:
        warnings.warn("Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ»Ğ¸Ñ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½. Ğ¡Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ holdout 20% Ğ¸Ğ· train.csv.")
        train_df, test_df = _make_holdout(train_df, target_column)
    
    # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
    task_mode = _map_task_type(task_type, metadata.get("task_details"))

    automl, oof_predictions = train_automl(
        train_df, target_column, task_mode, timeout, verbose=verbose, variant=variant
    )
    
    # ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ
    if verbose:
        print(f"ğŸ“ˆ ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ...")
    if verbose:
        test_metrics = evaluate_model(automl, test_df, target_column)
    else:
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning)
            test_metrics = evaluate_model(automl, test_df, target_column)
    
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°
    report = create_report(
        dataset_name, metadata, automl, oof_predictions, test_metrics, timeout, variant
    )
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
    report_path = save_report(dataset_name, report)
    
    if verbose:
        print(f"âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾! ĞÑ‚Ñ‡Ñ‘Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½: {report_path}")
        print(f"ğŸ† Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {report['automl']['best_model'].get('name', 'N/A')}")
        print(f"ğŸ“Š Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸: {test_metrics}")
    
    return {
        "dataset": dataset_name,
        "report_path": report_path,
        "test_metrics": test_metrics,
        "best_model": report["automl"]["best_model"],
        "variant": variant,
    }


def _map_task_type(task_type: str, task_details: Optional[Dict[str, Any]]) -> str:
    """ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ‚Ğ¸Ğ¿Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ LAMA (binary/multiclass/reg)."""
    if task_details:
        mode = str(task_details.get("mode", "")).lower()
        class_count = task_details.get("class_count")
        if mode == "binary" or class_count == 2:
            return "binary"
        if mode == "multiclass" or (isinstance(class_count, int) and class_count and class_count > 2):
            return "multiclass"
        if str(task_details.get("type", "")).lower().startswith("reg"):
            return "reg"

    task_lower = task_type.lower()
    if "binary" in task_lower:
        return "binary"
    if "multiclass" in task_lower:
        return "multiclass"
    if "reg" in task_lower:
        return "reg"
    # Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ
    return "binary"


def _primary_metric(task_type: str) -> str:
    """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ´Ğ»Ñ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."""
    return "rmse" if "reg" in task_type.lower() else "roc_auc"


def _build_fallback_leaderboard(automl: TabularAutoML) -> Optional[list[Dict[str, Any]]]:
    """Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ fallback-Ğ»Ğ¸Ğ´ĞµÑ€Ğ±Ğ¾Ñ€Ğ´ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ."""
    try:
        desc = automl.create_model_str_desc()
    except Exception as exc:
        warnings.warn(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LightAutoML: {exc}")
        return None

    entries: list[Dict[str, Any]] = []
    for line in desc.splitlines():
        line = line.strip()
        if not line or line.startswith("Final prediction"):
            continue
        if line.endswith("+"):
            line = line[:-1].rstrip()
        if "*" not in line:
            continue
        weight_part, model_part = line.split("*", 1)
        try:
            weight = float(weight_part.strip())
        except ValueError:
            continue

        model_part = model_part.strip()
        if model_part.startswith("(") and model_part.endswith(")"):
            model_part = model_part[1:-1]
        entries.append(
            {
                "model": model_part,
                "weight": weight,
            }
        )

    # Ğ¾Ñ‚ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ Ğ²ĞµÑÑƒ ÑƒĞ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ
    entries.sort(key=lambda x: x["weight"], reverse=True)
    return entries or None


def _make_holdout(
    df: pd.DataFrame,
    target_column: str,
    test_size: float = 0.2,
    random_state: int = 42,
) -> tuple[pd.DataFrame, pd.DataFrame]:
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ holdout-Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸."""
    if target_column not in df.columns:
        raise KeyError(f"Ğ¡Ñ‚Ğ¾Ğ»Ğ±ĞµÑ† Ñ‚Ğ°Ñ€Ğ³ĞµÑ‚Ğ° '{target_column}' Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….")

    target = df[target_column]
    stratify = None
    value_counts = target.value_counts(dropna=False)
    if value_counts.size > 1 and value_counts.min() >= 2:
        stratify = target

    train_part, test_part = train_test_split(
        df,
        test_size=test_size,
        stratify=stratify,
        random_state=random_state,
    )
    return train_part.reset_index(drop=True), test_part.reset_index(drop=True)

